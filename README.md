# Adapting Vision-Language Models Without Labels: A Comprehensive Survey [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

<div align="center">
<a href="https://arxiv.org/abs/2508.00000"><img src="https://img.shields.io/badge/arXiv-2508.00000-b31b1b.svg" alt="arXiv Badge"/></a>
<a href="https://github.com/tim-learn/awesome-unsupervised-learning-VLMs/stargazers"><img src="https://img.shields.io/github/stars/tim-learn/awesome-unsupervised-learning-VLMs" alt="Stars Badge"/></a>
<a href="https://github.com/tim-learn/awesome-unsupervised-learning-VLMs/network/members"><img src="https://img.shields.io/github/forks/tim-learn/awesome-unsupervised-learning-VLMs" alt="Forks Badge"/></a>
<a href="https://github.com/abhisheknaiidu/awesome-github-profile-readm/pulls"><img src="https://img.shields.io/github/issues-pr/tim-learn/awesome-unsupervised-learning-VLMs" alt="Pull Requests Badge"/></a>
<a href="https://github.com/tim-learn/awesome-unsupervised-learning-VLMs/issues"><img src="https://img.shields.io/github/issues/tim-learn/awesome-unsupervised-learning-VLMs" alt="Issues Badge"/></a>
<a href="https://github.com/tim-learn/awesome-unsupervised-learning-VLMs/blob/main/LICENSE"><img src="https://img.shields.io/github/license/tim-learn/awesome-unsupervised-learning-VLMs" alt="License Badge"/></a>
</div>

This repository collects research papers on **unsupervised learning methods with VLMs**. The repository will be continuously updated to track the latest work in the community. 

**Keywords: Vision Language Model, xx, xxx**

## :fire: Update
- [Aug 6th, 2025] xxx

## :page_with_curl: Overview
![avatar](task-taxonomy-fig.png)

## :sparkles: Contents
- [:spades: Data Free Transfer](cat/data_free_transfer.md)
- [:hearts: Unsupervised Domain Transfer](cat/unsupervised_domain_transfer.md)
- [:clubs: Episodic Test-Time Adaptation](cat/episodic_test_time_adaptation.md)
- [:diamonds: Online Test-Time Adaptation](cat/online_test_time_adaptation.md)

## ü§ù Citation
Please visit [Adapting Vision-Language Models Without Labels: A Comprehensive Survey](link) for more details and comprehensive information. If you find our paper and repo helpful, please consider citing it as follows:

```BibTeX
@article{dong2025adapting,
  title={Adapting Vision-Language Models Without Labels: A Comprehensive Survey}, 
  author={Dong, Hao and Sheng, Lijun and Liang, Jian and He, Ran and Chatzi, Eleni and Fink, Olga},
  journal={arXiv preprint arXiv:2508.xxxx}, 
  year={2025}}
```

## :collision: Selected Papers from Premier AI/ML Conferences

#### :spades: Data Free Transfer
* `ICML-2021` **[Learning Transferable Visual Models From Natural Language Supervision](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf)** [![Star](https://img.shields.io/github/stars/openai/CLIP.svg?style=social&label=Star)](https://github.com/openai/CLIP)
    * Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever

#### :hearts: Unsupervised Domain Transfer

#### :clubs: Episodic Test-Time Adaptation

#### :diamonds: Online Test-Time Adaptation